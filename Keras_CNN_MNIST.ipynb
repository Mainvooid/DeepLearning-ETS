{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras as K\n",
    "import pickle\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow,Theano,CNTK\n",
    "os.environ['KERAS_BACKEND'] = \"tensorflow\" #Use TF1,some incompatibilities with TF2.\n",
    "# Force one-gpu\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# Performance Improvement\n",
    "# Make sure channels-first (not last)\n",
    "K.backend.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batch(src):\n",
    "    '''Unpack the pickle files'''\n",
    "    with open(src, 'rb') as f:\n",
    "        if sys.version_info.major == 2:\n",
    "            data = pickle.load(f)\n",
    "        else:\n",
    "            data = pickle.load(f, encoding='latin1') # Contains the numpy array\n",
    "    return data\n",
    "\n",
    "def process_cifar():\n",
    "    '''Read data into RAM'''\n",
    "    print('Preparing train set...')\n",
    "    train_list = [read_batch('./cifar-10-batches-py/data_batch_{0}'.format(i + 1)) for i in range(5)]\n",
    "    x_train = np.concatenate([x['data'] for x in train_list])\n",
    "    y_train = np.concatenate([y['labels'] for y in train_list])\n",
    "    print('Preparing test set...')\n",
    "    tst = read_batch('./cifar-10-batches-py/test_batch')\n",
    "    x_test = tst['data']\n",
    "    y_test = np.asarray(tst['labels'])\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def maybe_download_cifar(src=\"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"):\n",
    "    '''Load the training and testing data'''\n",
    "    try:\n",
    "        return process_cifar()\n",
    "    except:\n",
    "        # Catch the exception that file doesn't exist & Download\n",
    "        print('Data does not exist. Downloading ' + src)\n",
    "        filename = src.split('/')[-1]\n",
    "        filepath = os.path.join(\"./\",filename)\n",
    "        def _recall_func(num,block_size,total_size):\n",
    "            sys.stdout.write('\\r>> downloading %s %.1f%%' % (filename,float(num*block_size)/float(total_size)*100.0))\n",
    "            sys.stdout.flush()\n",
    "        fname, h = urlretrieve(src, filepath,_recall_func)\n",
    "        file_info = os.stat(filepath)\n",
    "        print('Successfully download.',filename,file_info.st_size,'bytes')\n",
    "        print('Extracting files...')\n",
    "        with tarfile.open(fname) as tar:\n",
    "            tar.extractall()\n",
    "        os.remove(fname)\n",
    "        return process_cifar()\n",
    "    \n",
    "def cifar_for_library(channel_first=True, one_hot=False):\n",
    "    # Raw data\n",
    "    x_train, x_test, y_train, y_test = maybe_download_cifar()\n",
    "    # Scale pixel intensity\n",
    "    x_train = x_train / 255.0\n",
    "    x_test = x_test / 255.0\n",
    "    # Reshape\n",
    "    x_train = x_train.reshape(-1, 3, 32, 32)\n",
    "    x_test = x_test.reshape(-1, 3, 32, 32)\n",
    "    # Channel last\n",
    "    if not channel_first:\n",
    "        x_train = np.swapaxes(x_train, 1, 3)\n",
    "        x_test = np.swapaxes(x_test, 1, 3)\n",
    "    # One-hot encode y\n",
    "    if one_hot:\n",
    "        y_train = np.expand_dims(y_train, axis=-1)\n",
    "        y_test = np.expand_dims(y_test, axis=-1)\n",
    "        enc = OneHotEncoder(categorical_features='all')\n",
    "        fit = enc.fit(y_train)\n",
    "        y_train = fit.transform(y_train).toarray()\n",
    "        y_test = fit.transform(y_test).toarray()\n",
    "    # dtypes\n",
    "    x_train = x_train.astype(np.float32)\n",
    "    x_test = x_test.astype(np.float32)\n",
    "    y_train = y_train.astype(np.int32)\n",
    "    y_test = y_test.astype(np.int32)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "# Data into format for library\n",
    "x_train, x_test, y_train, y_test = cifar_for_library(channel_first=True, one_hot=True)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "print(x_train.dtype, x_test.dtype, y_train.dtype, y_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "EPOCHS  =  10\n",
    "BATCHSIZE  =  64\n",
    "LR  =  0.01\n",
    "MOMENTUM  =  0.9\n",
    "N_CLASSES  =  10\n",
    "GPU  =  True\n",
    "BATCH_SIZE  =  32\n",
    "\n",
    "def create_model(n_classes=N_CLASSES):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(50, kernel_size=(3, 3), padding='same', activation='relu',\n",
    "                     input_shape=(3, 32, 32)))\n",
    "    model.add(Conv2D(50, kernel_size=(3, 3), padding='same', activation='relu'))    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(100, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(100, kernel_size=(3, 3), padding='same', activation='relu'))    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def init_model(m, lr=LR, momentum=MOMENTUM):\n",
    "    m.compile(\n",
    "        loss = \"categorical_crossentropy\",\n",
    "        optimizer = K.optimizers.SGD(lr, momentum),\n",
    "        metrics = ['accuracy'])\n",
    "    return m\n",
    "\n",
    "model = create_model()\n",
    "model = init_model(model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size=BATCHSIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_guess = model.predict(x_test, batch_size=BATCHSIZE)\n",
    "y_guess = np.argmax(y_guess, axis=-1)\n",
    "y_truth = np.argmax(y_test, axis=-1)\n",
    "print(\"Accuracy: \", 1.*sum(y_guess == y_truth)/len(y_guess))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
